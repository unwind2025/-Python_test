{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T06:31:39.674876Z",
     "start_time": "2025-02-04T06:31:39.672152Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "#计算softmax举例\n",
    "x = torch.tensor([[100,200 ]], dtype=torch.float32)\n",
    "x_softmax = torch.softmax(x, dim=1)\n",
    "print(x_softmax)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7835e-44, 1.0000e+00]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:38:22.893027Z",
     "start_time": "2025-02-04T06:38:22.889261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1,2 ]], dtype=torch.float32)\n",
    "x_softmax = torch.softmax(x, dim=1)\n",
    "print(x_softmax)"
   ],
   "id": "14838e8eb306fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2689, 0.7311]])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "def custom_embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False):\n",
    "    \"\"\"\n",
    "    A simplified version of torch.nn.functional.embedding.\n",
    "    \"\"\"\n",
    "    # 输入检查\n",
    "    if not isinstance(input, torch.LongTensor):\n",
    "        raise TypeError(\"Input must be a LongTensor\")\n",
    "    \n",
    "    if padding_idx is not None:\n",
    "        if padding_idx >= weight.size(0) or padding_idx < 0:\n",
    "            raise ValueError(\"padding_idx must be within the range of weight size\")\n",
    "    \n",
    "    # 获取嵌入矩阵的形状\n",
    "    num_embeddings, embedding_dim = weight.size()\n",
    "    \n",
    "    # 初始化输出张量\n",
    "    output = torch.zeros(input.size() * (embedding_dim,), dtype=weight.dtype, device=weight.device)\n",
    "    \n",
    "    # 遍历输入张量的每个索引\n",
    "    for idx in input.view(-1).tolist():\n",
    "        if idx != padding_idx:\n",
    "            output.view(-1, embedding_dim)[idx] = weight[idx]\n",
    "    \n",
    "    # 如果指定了 max_norm，对嵌入向量进行归一化\n",
    "    if max_norm is not None:\n",
    "        with torch.no_grad():\n",
    "            norm = weight.norm(p=norm_type, dim=1, keepdim=True)\n",
    "            torch.clamp(norm, max=max_norm, out=norm)\n",
    "            weight.div_(norm)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 示例使用\n",
    "input_indices = torch.tensor([[1, 2, 4], [4, 3, 2]], dtype=torch.long)\n",
    "embedding_matrix = torch.randn(5, 10)  # 假设有 5 个词，每个词的嵌入维度为 10\n",
    "\n",
    "output = custom_embedding(input_indices, embedding_matrix)\n",
    "print(output)"
   ],
   "id": "7c25690d32918296"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
